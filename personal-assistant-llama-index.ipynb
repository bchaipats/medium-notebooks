{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38632505-3646-418f-8611-2f8d503435e4",
   "metadata": {},
   "source": [
    "# Author\n",
    "\n",
    "- Name: **Chaipat Suwannapoom**\n",
    "- Linkedin: https://www.linkedin.com/in/bchaipats/\n",
    "- GitHub: https://github.com/bchaipats\n",
    "- Medium: https://medium.com/@bchaipats\n",
    "- Twitter: https://twitter.com/bchaipats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d244b2-b0b0-4d33-a664-586334166313",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we'll be creating a personal assistant powered by open-source LLMs, free of charge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee03f2-db09-43e0-82d0-4033f39fabde",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3590c585-4839-4020-8f4a-2d87aaf79b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install langchain gpt4all llama-index sentence-transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aa47b-d6f6-4070-824c-1056ec95cbce",
   "metadata": {},
   "source": [
    "# Run LLM on a personal laptop\n",
    "\n",
    "If you worry about your data privacy (and your money), you might prefer running open-source LLMs privately on your local laptop instead of making API calls to external services like OpenAI.\n",
    "\n",
    "GPT4All helps us achieve this. We can browse and download open-source LLMs from the [GPT4All homepage](https://gpt4all.io/index.html) into our laptop. Say, we pick Mistral-7B since it's so cool and requires only 8 GB of RAM which is suitable for most consumer-grade CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcbc1bb-51f2-4740-98c0-24986e88d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download an open-source LLM\n",
    "!wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ced79b99-f624-4275-90c2-afeb9e0c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bangkok, which is also known as the City of Angels. It’s a bustling city with over 10 million people and it can be overwhelming at first glance. But don’t worry! I have put together this list of things to do in Bangkok for you so that your trip will be smooth sailing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "# Load the model\n",
    "llm = GPT4All(model='./mistral-7b-openorca.Q4_0.gguf')\n",
    "\n",
    "# Define a sentence to complete\n",
    "sentence = \"The capital of Thailand is \"\n",
    "\n",
    "# Complete the sentence\n",
    "response = llm.generate([sentence])\n",
    "print(response.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb5eee-962d-4335-a458-2ef4e7006345",
   "metadata": {},
   "source": [
    "# Personal data\n",
    "\n",
    "We'll be using a simple text file as our data source for simplicity. Later, we can just copy and paste texts from personal calendar, news, articles, books, etc. For example, here is a mock calendar for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd5944a8-28c5-4e0f-90e7-18b83a5c1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data.txt\n",
    "# data.txt\n",
    "\n",
    "19 Dec - Go to the movie\n",
    "20 Dec - Have a dinner with family\n",
    "21 Dec - Go to the birthday party\n",
    "22 Dec - Go to the dentist\n",
    "23 Dec - Finish writing a draft for blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebd3cf-0fb0-4dad-8b6b-8521f96307eb",
   "metadata": {},
   "source": [
    "# Create necessary components\n",
    "- **Embedding model** used to generate embedding vector representing the original text.\n",
    "- **Prompt helper** used to handle LLM context window and token limitations.\n",
    "- **Sentence splitter** used to split data into multiple chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4eb6d08-3e6e-4d88-81ee-10fa250bfbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcp/medium/medium-notebooks/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index import PromptHelper\n",
    "from llama_index.node_parser import SentenceSplitter\n",
    "\n",
    "# An embedding model used to structure text into representations\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# PromptHelper can help deal with LLM context window and token limitations\n",
    "prompt_helper = PromptHelper(context_window=2048)\n",
    "\n",
    "# SentenceSplitter used to split our data into multiple chunks\n",
    "# Only a number of relevant chunks will be retrieved and fed into LLMs\n",
    "node_parser = SentenceSplitter(chunk_size=300, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85908eeb-1b49-466d-8e76-dab5d2230928",
   "metadata": {},
   "source": [
    "# Create a container service\n",
    "This service packs the large language model, the embedding model, the prompt helper and the node parser into a single handy container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "563fb3bd-2bdb-4fe2-8d83-e55b79dea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import ServiceContext\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=embed_model,\n",
    "    prompt_helper=prompt_helper,\n",
    "    node_parser=node_parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f9670-bd1c-4dcb-919b-4848cd59b6b5",
   "metadata": {},
   "source": [
    "# Build an index and a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88cf1e8c-20e8-436d-80fe-64fcbddfb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SimpleDirectoryReader\n",
    "from llama_index import VectorStoreIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    document, service_context=service_context)\n",
    "\n",
    "# Build a query engine from the index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccfbc854-aa0a-472c-806f-daa983dc52c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your calendar for December is as follows:\n",
      "19 Dec - Go to the movie\n",
      "20 Dec - Have a dinner with family\n",
      "21 Dec - Go to the birthday party\n",
      "22 Dec - Go to the dentist\n",
      "23 Dec - Finish writing a draft for blog post\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Give me my calendar.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d82cea7d-4a71-4de3-b968-f93661440b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Dec is planned as \"Go to the dentist\".\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What's the plan for 22 Dec?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e73b78a-6dcc-42f6-9ab4-56bd1b9ab76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Dec\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('When should I finish a writing draft?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebb89d-6f25-4656-a245-e29c1cdd0984",
   "metadata": {},
   "source": [
    "# Summarize news\n",
    "\n",
    "For text summarization, it is better to create a new index that is suitable for the task. This is called a **SummaryIndex** which will synthesize an answer from all chunks of data compared to top-k chunks as in **VectorStoreIndex**.\n",
    "\n",
    "Credit: the following is the text from this [news](https://www.newsinlevels.com/products/grenade-in-the-face-level-3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "646594e9-491f-43e4-a230-104ace7f447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data.txt\n",
    "# data.txt\n",
    "\n",
    "Grenade in the face – level 3\n",
    "\n",
    "A soldier is lucky to be alive after a 40-millimetre grenade accidentally fired from a grenade launcher and embedded itself in his face.\n",
    "\n",
    "The terrible accident happened in Colombia and surgeons had to be especially careful with the operation.\n",
    "\n",
    "The soldier could not be moved to hospital by helicopter due to the risk of the explosive detonating, so an ambulance transferred him in an eight-hour ride to Bogota.\n",
    "\n",
    "The team of doctors had to improvise and operate outside the hospital to minimise impact on the building in case the grenade exploded. Luckily, the operation was a success and the soldier could reintegrate into his military unit and his family to make a full recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe35268b-40bb-4717-ba16-cb3f1c6ecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index import SummaryIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "summary_index = SummaryIndex.from_documents(\n",
    "    document, service_context=service_context)\n",
    "\n",
    "# Build a summary engine from the index\n",
    "summary_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74539139-e53e-40ad-a62d-b57f70c05072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A soldier in Colombia survived an accident where a grenade fired from a launcher embedded itself into his face. Due to the risk of explosion, he was transported by ambulance instead of helicopter and underwent surgery outside the hospital. The operation was successful, allowing him to recover and reintegrate with his military unit and family.\n"
     ]
    }
   ],
   "source": [
    "response = summary_engine.query(\"What is a summary of this collection of text?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
