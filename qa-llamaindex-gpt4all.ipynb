{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38632505-3646-418f-8611-2f8d503435e4",
   "metadata": {},
   "source": [
    "# Author\n",
    "\n",
    "- Name: **Chaipat Suwannapoom**\n",
    "- Linkedin: https://www.linkedin.com/in/bchaipats/\n",
    "- GitHub: https://github.com/bchaipats\n",
    "- Medium: https://medium.com/@bchaipats\n",
    "- Twitter: https://twitter.com/bchaipats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d244b2-b0b0-4d33-a664-586334166313",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this notebook, we'll be creating a personal assistant powered by open-source LLMs, free of charge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ee03f2-db09-43e0-82d0-4033f39fabde",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3590c585-4839-4020-8f4a-2d87aaf79b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain==0.1.9\n",
      "  Using cached langchain-0.1.9-py3-none-any.whl (816 kB)\n",
      "Collecting gpt4all==2.2.1\n",
      "  Using cached gpt4all-2.2.1-py3-none-macosx_10_15_universal2.whl (5.8 MB)\n",
      "Collecting llama-index==0.10.13\n",
      "  Using cached llama_index-0.10.13-py3-none-any.whl (5.6 kB)\n",
      "Collecting llama-index-embeddings-langchain==0.1.2\n",
      "  Using cached llama_index_embeddings_langchain-0.1.2-py3-none-any.whl (2.5 kB)\n",
      "Collecting llama-index-llms-langchain==0.1.3\n",
      "  Using cached llama_index_llms_langchain-0.1.3-py3-none-any.whl (4.6 kB)\n",
      "Collecting sentence-transformers==2.4.0\n",
      "  Using cached sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./.venv/lib/python3.11/site-packages (from langchain==0.1.9) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4\n",
      "  Using cached SQLAlchemy-2.0.27-cp311-cp311-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3\n",
      "  Using cached aiohttp-3.9.3-cp311-cp311-macosx_11_0_arm64.whl (387 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7\n",
      "  Using cached dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33\n",
      "  Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Collecting langchain-community<0.1,>=0.0.21\n",
      "  Using cached langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
      "Collecting langchain-core<0.2,>=0.1.26\n",
      "  Using cached langchain_core-0.1.27-py3-none-any.whl (250 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0\n",
      "  Using cached langsmith-0.1.10-py3-none-any.whl (63 kB)\n",
      "Collecting numpy<2,>=1\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Collecting pydantic<3,>=1\n",
      "  Using cached pydantic-2.6.3-py3-none-any.whl (395 kB)\n",
      "Requirement already satisfied: requests<3,>=2 in ./.venv/lib/python3.11/site-packages (from langchain==0.1.9) (2.31.0)\n",
      "Collecting tenacity<9.0.0,>=8.1.0\n",
      "  Using cached tenacity-8.2.3-py3-none-any.whl (24 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Collecting llama-index-agent-openai<0.2.0,>=0.1.4\n",
      "  Using cached llama_index_agent_openai-0.1.5-py3-none-any.whl (12 kB)\n",
      "Collecting llama-index-cli<0.2.0,>=0.1.2\n",
      "  Using cached llama_index_cli-0.1.5-py3-none-any.whl (25 kB)\n",
      "Collecting llama-index-core<0.11.0,>=0.10.13\n",
      "  Using cached llama_index_core-0.10.13-py3-none-any.whl (15.3 MB)\n",
      "Collecting llama-index-embeddings-openai<0.2.0,>=0.1.5\n",
      "  Using cached llama_index_embeddings_openai-0.1.6-py3-none-any.whl (6.0 kB)\n",
      "Collecting llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2\n",
      "  Using cached llama_index_indices_managed_llama_cloud-0.1.3-py3-none-any.whl (6.6 kB)\n",
      "Collecting llama-index-legacy<0.10.0,>=0.9.48\n",
      "  Using cached llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
      "Collecting llama-index-llms-openai<0.2.0,>=0.1.5\n",
      "  Using cached llama_index_llms_openai-0.1.6-py3-none-any.whl (9.4 kB)\n",
      "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3\n",
      "  Using cached llama_index_multi_modal_llms_openai-0.1.4-py3-none-any.whl (5.8 kB)\n",
      "Collecting llama-index-program-openai<0.2.0,>=0.1.3\n",
      "  Using cached llama_index_program_openai-0.1.4-py3-none-any.whl (4.1 kB)\n",
      "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2\n",
      "  Using cached llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
      "Collecting llama-index-readers-file<0.2.0,>=0.1.4\n",
      "  Using cached llama_index_readers_file-0.1.6-py3-none-any.whl (34 kB)\n",
      "Collecting llama-index-readers-llama-parse<0.2.0,>=0.1.2\n",
      "  Using cached llama_index_readers_llama_parse-0.1.3-py3-none-any.whl (2.5 kB)\n",
      "Collecting llama-index-llms-anyscale<0.2.0,>=0.1.1\n",
      "  Using cached llama_index_llms_anyscale-0.1.3-py3-none-any.whl (4.0 kB)\n",
      "Collecting transformers<5.0.0,>=4.32.0\n",
      "  Using cached transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "Collecting torch>=1.11.0\n",
      "  Using cached torch-2.2.1-cp311-none-macosx_11_0_arm64.whl (59.7 MB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.1.post1-cp311-cp311-macosx_12_0_arm64.whl (10.4 MB)\n",
      "Collecting scipy\n",
      "  Using cached scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl (31.4 MB)\n",
      "Collecting huggingface-hub>=0.15.1\n",
      "  Using cached huggingface_hub-0.21.1-py3-none-any.whl (346 kB)\n",
      "Collecting Pillow\n",
      "  Using cached pillow-10.2.0-cp311-cp311-macosx_11_0_arm64.whl (3.3 MB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Using cached aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.1.9) (23.2.0)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Using cached frozenlist-1.4.1-cp311-cp311-macosx_11_0_arm64.whl (53 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Using cached multidict-6.0.5-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Using cached yarl-1.9.4-cp311-cp311-macosx_11_0_arm64.whl (81 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0\n",
      "  Using cached marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0\n",
      "  Using cached typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting fsspec>=2023.5.0\n",
      "  Using cached fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in ./.venv/lib/python3.11/site-packages (from huggingface-hub>=0.15.1->sentence-transformers==2.4.0) (23.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./.venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.1.9) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./.venv/lib/python3.11/site-packages (from langchain-core<0.2,>=0.1.26->langchain==0.1.9) (4.3.0)\n",
      "Collecting orjson<4.0.0,>=3.9.14\n",
      "  Using cached orjson-3.9.15-cp311-cp311-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "Collecting llama-index-vector-stores-chroma<0.2.0,>=0.1.1\n",
      "  Using cached llama_index_vector_stores_chroma-0.1.4-py3-none-any.whl (4.5 kB)\n",
      "Collecting deprecated>=1.2.9.3\n",
      "  Using cached Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8\n",
      "  Using cached dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: httpx in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (0.27.0)\n",
      "Collecting llamaindex-py-client<0.2.0,>=0.1.13\n",
      "  Using cached llamaindex_py_client-0.1.13-py3-none-any.whl (107 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./.venv/lib/python3.11/site-packages (from llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (1.6.0)\n",
      "Collecting networkx>=3.0\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting nltk<4.0.0,>=3.8.1\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting openai>=1.1.0\n",
      "  Using cached openai-1.12.0-py3-none-any.whl (226 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.1-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Collecting tiktoken>=0.3.3\n",
      "  Using cached tiktoken-0.6.0-cp311-cp311-macosx_11_0_arm64.whl (949 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in ./.venv/lib/python3.11/site-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.13) (4.12.3)\n",
      "Collecting bs4<0.0.3,>=0.0.2\n",
      "  Using cached bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting pymupdf<2.0.0,>=1.23.21\n",
      "  Using cached PyMuPDF-1.23.25-cp311-none-macosx_11_0_arm64.whl (3.7 MB)\n",
      "Collecting pypdf<5.0.0,>=4.0.1\n",
      "  Using cached pypdf-4.0.2-py3-none-any.whl (283 kB)\n",
      "Collecting llama-parse<0.4.0,>=0.3.3\n",
      "  Using cached llama_parse-0.3.4-py3-none-any.whl (5.3 kB)\n",
      "Collecting annotated-types>=0.4.0\n",
      "  Using cached annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Collecting pydantic-core==2.16.3\n",
      "  Using cached pydantic_core-2.16.3-cp311-cp311-macosx_11_0_arm64.whl (1.7 MB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.9) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.9) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.9) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.11/site-packages (from requests<3,>=2->langchain==0.1.9) (2024.2.2)\n",
      "Collecting sympy\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers==2.4.0) (3.1.3)\n",
      "Collecting regex!=2019.12.17\n",
      "  Using cached regex-2023.12.25-cp311-cp311-macosx_11_0_arm64.whl (291 kB)\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-macosx_11_0_arm64.whl (2.4 MB)\n",
      "Collecting safetensors>=0.4.1\n",
      "  Using cached safetensors-0.4.2-cp311-cp311-macosx_11_0_arm64.whl (393 kB)\n",
      "Collecting joblib>=1.2.0\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.11/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain==0.1.9) (1.3.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in ./.venv/lib/python3.11/site-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index==0.10.13) (2.5)\n",
      "Collecting wrapt<2,>=1.10\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Collecting chromadb<0.5.0,>=0.4.22\n",
      "  Using cached chromadb-0.4.24-py3-none-any.whl (525 kB)\n",
      "Collecting onnxruntime<2.0.0,>=1.17.0\n",
      "  Using cached onnxruntime-1.17.1-cp311-cp311-macosx_11_0_universal2.whl (14.8 MB)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.11/site-packages (from httpx->llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (0.14.0)\n",
      "Collecting click\n",
      "  Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Collecting distro<2,>=1.7.0\n",
      "  Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Collecting PyMuPDFb==1.23.22\n",
      "  Using cached PyMuPDFb-1.23.22-py3-none-macosx_11_0_arm64.whl (29.4 MB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.3-cp311-cp311-macosx_11_0_universal2.whl (271 kB)\n",
      "Collecting mypy-extensions>=0.3.0\n",
      "  Using cached mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers==2.4.0) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.11/site-packages (from pandas->llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (2.8.2)\n",
      "Collecting pytz>=2020.1\n",
      "  Using cached pytz-2024.1-py2.py3-none-any.whl (505 kB)\n",
      "Collecting tzdata>=2022.7\n",
      "  Using cached tzdata-2024.1-py2.py3-none-any.whl (345 kB)\n",
      "Collecting mpmath>=0.19\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Collecting build>=1.0.3\n",
      "  Using cached build-1.0.3-py3-none-any.whl (18 kB)\n",
      "Collecting chroma-hnswlib==0.7.3\n",
      "  Using cached chroma_hnswlib-0.7.3-cp311-cp311-macosx_11_0_arm64.whl (198 kB)\n",
      "Collecting fastapi>=0.95.2\n",
      "  Using cached fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
      "Collecting uvicorn[standard]>=0.18.3\n",
      "  Using cached uvicorn-0.27.1-py3-none-any.whl (60 kB)\n",
      "Collecting posthog>=2.4.0\n",
      "  Using cached posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n",
      "Collecting pulsar-client>=3.1.0\n",
      "  Using cached pulsar_client-3.4.0-cp311-cp311-macosx_10_15_universal2.whl (10.9 MB)\n",
      "Collecting opentelemetry-api>=1.2.0\n",
      "  Using cached opentelemetry_api-1.23.0-py3-none-any.whl (58 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_grpc-1.23.0-py3-none-any.whl (18 kB)\n",
      "Collecting opentelemetry-instrumentation-fastapi>=0.41b0\n",
      "  Using cached opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl (11 kB)\n",
      "Collecting opentelemetry-sdk>=1.2.0\n",
      "  Using cached opentelemetry_sdk-1.23.0-py3-none-any.whl (105 kB)\n",
      "Collecting pypika>=0.48.9\n",
      "  Using cached PyPika-0.48.9-py2.py3-none-any.whl\n",
      "Requirement already satisfied: overrides>=7.3.1 in ./.venv/lib/python3.11/site-packages (from chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index==0.10.13) (7.7.0)\n",
      "Collecting importlib-resources\n",
      "  Using cached importlib_resources-6.1.2-py3-none-any.whl (34 kB)\n",
      "Collecting grpcio>=1.58.0\n",
      "  Using cached grpcio-1.62.0-cp311-cp311-macosx_10_10_universal2.whl (10.0 MB)\n",
      "Collecting bcrypt>=4.0.1\n",
      "  Using cached bcrypt-4.1.2-cp39-abi3-macosx_10_12_universal2.whl (528 kB)\n",
      "Collecting typer>=0.9.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Collecting kubernetes>=28.1.0\n",
      "  Using cached kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n",
      "Collecting mmh3>=4.0.1\n",
      "  Using cached mmh3-4.1.0-cp311-cp311-macosx_11_0_arm64.whl (30 kB)\n",
      "Collecting coloredlogs\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Collecting flatbuffers\n",
      "  Using cached flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Collecting protobuf\n",
      "  Using cached protobuf-4.25.3-cp37-abi3-macosx_10_9_universal2.whl (394 kB)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.13->llama-index==0.10.13) (1.16.0)\n",
      "Collecting pyproject_hooks\n",
      "  Using cached pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting starlette<0.37.0,>=0.36.3\n",
      "  Using cached starlette-0.36.3-py3-none-any.whl (71 kB)\n",
      "Collecting google-auth>=1.0.1\n",
      "  Using cached google_auth-2.28.1-py2.py3-none-any.whl (186 kB)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in ./.venv/lib/python3.11/site-packages (from kubernetes>=28.1.0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index==0.10.13) (1.7.0)\n",
      "Collecting requests-oauthlib\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting oauthlib>=3.2.2\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Collecting importlib-metadata<7.0,>=6.0\n",
      "  Using cached importlib_metadata-6.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting googleapis-common-protos~=1.52\n",
      "  Using cached googleapis_common_protos-1.62.0-py2.py3-none-any.whl (228 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.23.0\n",
      "  Using cached opentelemetry_exporter_otlp_proto_common-1.23.0-py3-none-any.whl (17 kB)\n",
      "Collecting opentelemetry-proto==1.23.0\n",
      "  Using cached opentelemetry_proto-1.23.0-py3-none-any.whl (50 kB)\n",
      "Collecting opentelemetry-instrumentation-asgi==0.44b0\n",
      "  Using cached opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl (14 kB)\n",
      "Collecting opentelemetry-instrumentation==0.44b0\n",
      "  Using cached opentelemetry_instrumentation-0.44b0-py3-none-any.whl (28 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.44b0\n",
      "  Using cached opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl (36 kB)\n",
      "Collecting opentelemetry-util-http==0.44b0\n",
      "  Using cached opentelemetry_util_http-0.44b0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools>=16.0 in ./.venv/lib/python3.11/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb<0.5.0,>=0.4.22->llama-index-vector-stores-chroma<0.2.0,>=0.1.1->llama-index-cli<0.2.0,>=0.1.2->llama-index==0.10.13) (65.5.0)\n",
      "Collecting asgiref~=3.0\n",
      "  Using cached asgiref-3.7.2-py3-none-any.whl (24 kB)\n",
      "Collecting monotonic>=1.5\n",
      "  Using cached monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting backoff>=1.10.0\n",
      "  Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Collecting httptools>=0.5.0\n",
      "  Using cached httptools-0.6.1-cp311-cp311-macosx_10_9_universal2.whl (145 kB)\n",
      "Collecting python-dotenv>=0.13\n",
      "  Using cached python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0\n",
      "  Using cached uvloop-0.19.0-cp311-cp311-macosx_10_9_universal2.whl (1.4 MB)\n",
      "Collecting watchfiles>=0.13\n",
      "  Using cached watchfiles-0.21.0-cp311-cp311-macosx_11_0_arm64.whl (418 kB)\n",
      "Collecting websockets>=10.4\n",
      "  Using cached websockets-12.0-cp311-cp311-macosx_11_0_arm64.whl (121 kB)\n",
      "Collecting humanfriendly>=9.1\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.3-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting zipp>=0.5\n",
      "  Using cached zipp-3.17.0-py3-none-any.whl (7.4 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
      "Installing collected packages: pytz, pypika, mpmath, monotonic, mmh3, flatbuffers, dirtyjson, zipp, wrapt, websockets, uvloop, tzdata, typing-extensions, tqdm, threadpoolctl, tenacity, sympy, safetensors, regex, python-dotenv, pyproject_hooks, pypdf, PyMuPDFb, pyasn1, pulsar-client, protobuf, Pillow, orjson, opentelemetry-util-http, opentelemetry-semantic-conventions, oauthlib, numpy, networkx, mypy-extensions, multidict, marshmallow, jsonpatch, joblib, importlib-resources, humanfriendly, httptools, grpcio, greenlet, fsspec, frozenlist, filelock, distro, click, cachetools, bcrypt, backoff, asgiref, annotated-types, yarl, watchfiles, uvicorn, typing-inspect, typer, torch, tiktoken, starlette, SQLAlchemy, scipy, rsa, requests-oauthlib, pymupdf, pydantic-core, pyasn1-modules, posthog, pandas, opentelemetry-proto, nltk, importlib-metadata, huggingface-hub, gpt4all, googleapis-common-protos, deprecated, coloredlogs, chroma-hnswlib, build, bs4, aiosignal, tokenizers, scikit-learn, pydantic, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, onnxruntime, google-auth, dataclasses-json, aiohttp, transformers, opentelemetry-sdk, opentelemetry-instrumentation, openai, llamaindex-py-client, langsmith, kubernetes, fastapi, sentence-transformers, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-grpc, llama-index-legacy, llama-index-core, langchain-core, opentelemetry-instrumentation-fastapi, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-embeddings-langchain, langchain-community, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-llms-anyscale, llama-index-agent-openai, langchain, chromadb, llama-index-vector-stores-chroma, llama-index-program-openai, llama-index-llms-langchain, llama-index-question-gen-openai, llama-index-cli, llama-index\n",
      "Successfully installed Pillow-10.2.0 PyMuPDFb-1.23.22 SQLAlchemy-2.0.27 aiohttp-3.9.3 aiosignal-1.3.1 annotated-types-0.6.0 asgiref-3.7.2 backoff-2.2.1 bcrypt-4.1.2 bs4-0.0.2 build-1.0.3 cachetools-5.3.3 chroma-hnswlib-0.7.3 chromadb-0.4.24 click-8.1.7 coloredlogs-15.0.1 dataclasses-json-0.6.4 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 fastapi-0.110.0 filelock-3.13.1 flatbuffers-23.5.26 frozenlist-1.4.1 fsspec-2024.2.0 google-auth-2.28.1 googleapis-common-protos-1.62.0 gpt4all-2.2.1 greenlet-3.0.3 grpcio-1.62.0 httptools-0.6.1 huggingface-hub-0.21.1 humanfriendly-10.0 importlib-metadata-6.11.0 importlib-resources-6.1.2 joblib-1.3.2 jsonpatch-1.33 kubernetes-29.0.0 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.27 langsmith-0.1.10 llama-index-0.10.13 llama-index-agent-openai-0.1.5 llama-index-cli-0.1.5 llama-index-core-0.10.13 llama-index-embeddings-langchain-0.1.2 llama-index-embeddings-openai-0.1.6 llama-index-indices-managed-llama-cloud-0.1.3 llama-index-legacy-0.9.48 llama-index-llms-anyscale-0.1.3 llama-index-llms-langchain-0.1.3 llama-index-llms-openai-0.1.6 llama-index-multi-modal-llms-openai-0.1.4 llama-index-program-openai-0.1.4 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.6 llama-index-readers-llama-parse-0.1.3 llama-index-vector-stores-chroma-0.1.4 llama-parse-0.3.4 llamaindex-py-client-0.1.13 marshmallow-3.21.0 mmh3-4.1.0 monotonic-1.6 mpmath-1.3.0 multidict-6.0.5 mypy-extensions-1.0.0 networkx-3.2.1 nltk-3.8.1 numpy-1.26.4 oauthlib-3.2.2 onnxruntime-1.17.1 openai-1.12.0 opentelemetry-api-1.23.0 opentelemetry-exporter-otlp-proto-common-1.23.0 opentelemetry-exporter-otlp-proto-grpc-1.23.0 opentelemetry-instrumentation-0.44b0 opentelemetry-instrumentation-asgi-0.44b0 opentelemetry-instrumentation-fastapi-0.44b0 opentelemetry-proto-1.23.0 opentelemetry-sdk-1.23.0 opentelemetry-semantic-conventions-0.44b0 opentelemetry-util-http-0.44b0 orjson-3.9.15 pandas-2.2.1 posthog-3.4.2 protobuf-4.25.3 pulsar-client-3.4.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pydantic-2.6.3 pydantic-core-2.16.3 pymupdf-1.23.25 pypdf-4.0.2 pypika-0.48.9 pyproject_hooks-1.0.0 python-dotenv-1.0.1 pytz-2024.1 regex-2023.12.25 requests-oauthlib-1.3.1 rsa-4.9 safetensors-0.4.2 scikit-learn-1.4.1.post1 scipy-1.12.0 sentence-transformers-2.4.0 starlette-0.36.3 sympy-1.12 tenacity-8.2.3 threadpoolctl-3.3.0 tiktoken-0.6.0 tokenizers-0.15.2 torch-2.2.1 tqdm-4.66.2 transformers-4.38.1 typer-0.9.0 typing-extensions-4.10.0 typing-inspect-0.9.0 tzdata-2024.1 uvicorn-0.27.1 uvloop-0.19.0 watchfiles-0.21.0 websockets-12.0 wrapt-1.16.0 yarl-1.9.4 zipp-3.17.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install \\\n",
    "    langchain==0.1.9 \\\n",
    "    gpt4all==2.2.1 \\\n",
    "    llama-index==0.10.13 \\\n",
    "    llama-index-embeddings-langchain==0.1.2 \\\n",
    "    llama-index-llms-langchain==0.1.3 \\\n",
    "    sentence-transformers==2.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82aa47b-d6f6-4070-824c-1056ec95cbce",
   "metadata": {},
   "source": [
    "# Run LLM on a personal laptop\n",
    "\n",
    "If you worry about your data privacy (and your money), you might prefer running open-source LLMs privately on your local laptop instead of making API calls to external services like OpenAI.\n",
    "\n",
    "GPT4All helps us achieve this. We can browse and download open-source LLMs from the [GPT4All homepage](https://gpt4all.io/index.html) into our laptop. Say, we pick Mistral-7B since it's so cool and requires only 8 GB of RAM which is suitable for most consumer-grade CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9bcbc1bb-51f2-4740-98c0-24986e88d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-02-28 09:57:30--  https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf\n",
      "Resolving gpt4all.io (gpt4all.io)... 104.26.0.159, 104.26.1.159, 172.67.71.169\n",
      "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4108927744 (3.8G)\n",
      "Saving to: ‘mistral-7b-openorca.Q4_0.gguf’\n",
      "\n",
      "mistral-7b-openorca  93%[=================>  ]   3.60G  54.9MB/s    in 66s     \n",
      "\n",
      "2024-02-28 09:58:37 (56.0 MB/s) - Connection closed at byte 3861454848. Retrying.\n",
      "\n",
      "--2024-02-28 09:58:38--  (try: 2)  https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf\n",
      "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 4108927744 (3.8G), 247472896 (236M) remaining\n",
      "Saving to: ‘mistral-7b-openorca.Q4_0.gguf’\n",
      "\n",
      "mistral-7b-openorca  94%[++++++++++++++++++  ]   3.63G  95.4KB/s    in 4m 10s  \n",
      "\n",
      "2024-02-28 10:02:51 (137 KB/s) - Connection closed at byte 3896442880. Retrying.\n",
      "\n",
      "--2024-02-28 10:02:53--  (try: 3)  https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf\n",
      "Connecting to gpt4all.io (gpt4all.io)|104.26.0.159|:443... connected.\n",
      "HTTP request sent, awaiting response... 206 Partial Content\n",
      "Length: 4108927744 (3.8G), 212484864 (203M) remaining\n",
      "Saving to: ‘mistral-7b-openorca.Q4_0.gguf’\n",
      "\n",
      "mistral-7b-openorca 100%[++++++++++++++++++=>]   3.83G  22.7MB/s    in 10s     \n",
      "\n",
      "2024-02-28 10:03:05 (19.6 MB/s) - ‘mistral-7b-openorca.Q4_0.gguf’ saved [4108927744/4108927744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download an open-source LLM\n",
    "!wget https://gpt4all.io/models/gguf/mistral-7b-openorca.Q4_0.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced79b99-f624-4275-90c2-afeb9e0c5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bangkok, which is also known as the City of Angels. It’s a bustling city with over 10 million people and it can be overwhelming at first glance. But don’t worry! I have put together this list of things to do in Bangkok for you so that your trip will be smooth sailing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import GPT4All\n",
    "\n",
    "# Load the model\n",
    "llm = GPT4All(model='./mistral-7b-openorca.Q4_0.gguf')\n",
    "\n",
    "# Define a sentence to complete\n",
    "sentence = \"The capital of Thailand is \"\n",
    "\n",
    "# Complete the sentence\n",
    "response = llm.generate([sentence])\n",
    "print(response.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cb5eee-962d-4335-a458-2ef4e7006345",
   "metadata": {},
   "source": [
    "# Personal data\n",
    "\n",
    "We'll be using a simple text file as our data source for simplicity. Later, we can just copy and paste texts from personal calendar, news, articles, books, etc. For example, here is a mock calendar for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5944a8-28c5-4e0f-90e7-18b83a5c1d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data.txt\n",
    "# data.txt\n",
    "\n",
    "19 Dec - Go to the movie\n",
    "20 Dec - Have a dinner with family\n",
    "21 Dec - Go to the birthday party\n",
    "22 Dec - Go to the dentist\n",
    "23 Dec - Finish writing a draft for blog post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eebd3cf-0fb0-4dad-8b6b-8521f96307eb",
   "metadata": {},
   "source": [
    "# Create necessary components\n",
    "- **Embedding model** used to generate embedding vector representing the original text.\n",
    "- **Prompt helper** used to handle LLM context window and token limitations.\n",
    "- **Sentence splitter** used to split data into multiple chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4eb6d08-3e6e-4d88-81ee-10fa250bfbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcp/notebooks/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from llama_index.embeddings.langchain import LangchainEmbedding\n",
    "from llama_index.core.indices.prompt_helper import PromptHelper\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# An embedding model used to structure text into representations\n",
    "embed_model = LangchainEmbedding(\n",
    "    HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    ")\n",
    "\n",
    "# PromptHelper can help deal with LLM context window and token limitations\n",
    "prompt_helper = PromptHelper(context_window=2048)\n",
    "\n",
    "# SentenceSplitter used to split our data into multiple chunks\n",
    "# Only a number of relevant chunks will be retrieved and fed into LLMs\n",
    "node_parser = SentenceSplitter(chunk_size=300, chunk_overlap=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85908eeb-1b49-466d-8e76-dab5d2230928",
   "metadata": {},
   "source": [
    "# Create a container service\n",
    "This service packs the large language model, the embedding model, the prompt helper and the node parser into a single handy container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4725844-a18e-49b7-9be4-ecda43f14c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7fb6893-9ede-4a57-b3c0-7b574472e101",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model\n",
    "Settings.prompt_helper = prompt_helper\n",
    "Settings.node_parser = node_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605f9670-bd1c-4dcb-919b-4848cd59b6b5",
   "metadata": {},
   "source": [
    "# Build an index and a query engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88cf1e8c-20e8-436d-80fe-64fcbddfb7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "index = VectorStoreIndex.from_documents(document)\n",
    "\n",
    "# Build a query engine from the index\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccfbc854-aa0a-472c-806f-daa983dc52c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bcp/notebooks/.venv/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `predict` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Your calendar for December is as follows:\n",
      "19 Dec - Go to the movie\n",
      "20 Dec - Have a dinner with family\n",
      "21 Dec - Go to the birthday party\n",
      "22 Dec - Go to the dentist\n",
      "23 Dec - Finish writing a draft for blog post\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('Give me my calendar.')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d82cea7d-4a71-4de3-b968-f93661440b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 Dec is planned as \"Go to the dentist\".\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query(\"What's the plan for 22 Dec?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e73b78a-6dcc-42f6-9ab4-56bd1b9ab76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 Dec\n"
     ]
    }
   ],
   "source": [
    "response = query_engine.query('When should I finish a writing draft?')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ebb89d-6f25-4656-a245-e29c1cdd0984",
   "metadata": {},
   "source": [
    "# Summarize news\n",
    "\n",
    "For text summarization, it is better to create a new index that is suitable for the task. This is called a **SummaryIndex** which will synthesize an answer from all chunks of data compared to top-k chunks as in **VectorStoreIndex**.\n",
    "\n",
    "Credit: the following is the text from this [news](https://www.newsinlevels.com/products/grenade-in-the-face-level-3/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "646594e9-491f-43e4-a230-104ace7f447f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./data.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./data.txt\n",
    "# data.txt\n",
    "\n",
    "Grenade in the face – level 3\n",
    "\n",
    "A soldier is lucky to be alive after a 40-millimetre grenade accidentally fired from a grenade launcher and embedded itself in his face.\n",
    "\n",
    "The terrible accident happened in Colombia and surgeons had to be especially careful with the operation.\n",
    "\n",
    "The soldier could not be moved to hospital by helicopter due to the risk of the explosive detonating, so an ambulance transferred him in an eight-hour ride to Bogota.\n",
    "\n",
    "The team of doctors had to improvise and operate outside the hospital to minimise impact on the building in case the grenade exploded. Luckily, the operation was a success and the soldier could reintegrate into his military unit and his family to make a full recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe35268b-40bb-4717-ba16-cb3f1c6ecf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import SummaryIndex\n",
    "\n",
    "# Load data.txt into a document\n",
    "document = SimpleDirectoryReader(input_files=['./data.txt']).load_data()\n",
    "\n",
    "# Process data (chunking, embedding, indexing) and store them\n",
    "summary_index = SummaryIndex.from_documents(document)\n",
    "\n",
    "# Build a summary engine from the index\n",
    "summary_engine = summary_index.as_query_engine(response_mode=\"tree_summarize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74539139-e53e-40ad-a62d-b57f70c05072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A soldier in Colombia survived an accident where a grenade fired from a launcher embedded itself into his face. Due to the risk of explosion, he was transported by ambulance instead of helicopter and underwent surgery outside the hospital. The operation was successful, allowing him to recover and reintegrate with his military unit and family.\n"
     ]
    }
   ],
   "source": [
    "response = summary_engine.query(\"What is a summary of this collection of text?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef335c53-ab96-4bd5-99dc-6d0860966dcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
